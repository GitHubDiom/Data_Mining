{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "import nltk  \n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self ,X , y = None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return [{ word : True for word in word_tokenize(document) } for document in X ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "input_filename  = os.path.join(os.path.expanduser(\"~\"),\"For Data Analysis\",\"Data_Mining\",\"Chapter06\",\"Dataset\",\"python_tweets.json\")\n",
    "label_filename = os.path.join(os.path.expanduser(\"~\"),\"For Data Analysis\",\"Data_Mining\",\"Chapter06\",\"Dataset\",\"python_classes.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "with open(input_filename) as inf:\n",
    "    for line in inf:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tweets.append(json.loads(line)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_filename) as inf:\n",
    "    labels = json.load(inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "'''\n",
    "此处流水线的作用是先将tweets数据集转化为一个词袋（出现的词则标记为TRUE）,后将词袋转化为可被分类器处理的矩阵(每行为不同文本，每列为不同的单词),\n",
    "最后将数据集使用二元伯努利概型预测正确性\n",
    "\n",
    "'''\n",
    "pipeline = Pipeline([(\"split_words\", NLTKBOW()),(\"vectorize\", DictVectorizer() ),(\"naive_bayes\", BernoulliNB() )])\n",
    "scores = cross_val_score(pipeline , tweets , labels, scoring = 'f1')#默认会返回类别为1的f1值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.60735171261487"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(scores)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.33333333333333"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#根据自己理解模拟pipeline过程， 但结果不正确，pipeline过程和estimator仍需理解原理\n",
    "NLTKBOW().fit_transform(tweets,None)\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(NLTKBOW().fit_transform(tweets,None)).toarray()\n",
    "nb = BernoulliNB()\n",
    "scores = cross_val_score(nb ,vec.fit_transform(NLTKBOW().fit_transform(tweets,None)).toarray(),labels,scoring = 'f1')\n",
    "np.mean(scores)*100\n",
    "#print(vec.fit_transform(NLTKBOW().fit_transform(tweets,None)).toarray().shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "从模型中获取前k个有用的特征\n",
    "'''\n",
    "model = pipeline.fit(tweets , labels)\n",
    "nb = model.named_steps['naive_bayes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 0.7962962962962962\n",
      "1 @ 0.685185185185185\n",
      "2 https 0.574074074074074\n",
      "3 RT 0.5555555555555556\n",
      "4 Python 0.4259259259259259\n",
      "5 # 0.40740740740740744\n",
      "6 . 0.35185185185185175\n",
      "7 , 0.2592592592592592\n",
      "8 in 0.2592592592592592\n",
      "9 to 0.2407407407407407\n",
      "10 Developer 0.2037037037037037\n",
      "11 nodejs 0.2037037037037037\n",
      "12 js 0.2037037037037037\n",
      "13 with 0.2037037037037037\n",
      "14 ( 0.16666666666666669\n",
      "15 the 0.16666666666666669\n",
      "16 ! 0.16666666666666669\n",
      "17 ) 0.1481481481481481\n",
      "18 a 0.1481481481481481\n",
      "19 I 0.12962962962962962\n",
      "20 node 0.12962962962962962\n",
      "21 BoostlogHQ 0.12962962962962962\n",
      "22 angularjs 0.12962962962962962\n",
      "23 coding 0.12962962962962962\n",
      "24 of 0.12962962962962962\n",
      "25 and 0.11111111111111109\n",
      "26 you 0.11111111111111109\n",
      "27 is 0.11111111111111109\n",
      "28 Facebook 0.09259259259259259\n",
      "29 for 0.09259259259259259\n",
      "30 10 0.09259259259259259\n",
      "31 Top 0.09259259259259259\n",
      "32 100DaysOfCode 0.09259259259259259\n",
      "33 Javascript 0.09259259259259259\n",
      "34 Make 0.09259259259259259\n",
      "35 webdev 0.09259259259259259\n",
      "36 projects 0.09259259259259259\n",
      "37 ? 0.09259259259259259\n",
      "38 python 0.09259259259259259\n",
      "39 How 0.09259259259259259\n",
      "40 Bot 0.09259259259259259\n",
      "41 //t.co/mJjYGzvVjU 0.09259259259259259\n",
      "42 [ 0.07407407407407407\n",
      "43 PHP 0.07407407407407407\n",
      "44 popular 0.07407407407407407\n",
      "45 write 0.07407407407407407\n",
      "46 Monty 0.07407407407407407\n",
      "47 started 0.07407407407407407\n",
      "48 on 0.07407407407407407\n",
      "49 ] 0.07407407407407407\n"
     ]
    }
   ],
   "source": [
    "feature_probabilities = nb.feature_log_prob_#将特征出现的概率对数化（由乘法转化为加法，防止乘0）\n",
    "'''\n",
    "x = np.array([3, 1, 2])\n",
    "np.argsort(x)#返回a[1]<a[2]<a[0]即，1<2<3\n",
    ">>>array([1, 2, 0], dtype=int64)\n",
    "\n",
    "'''\n",
    "top_feature = np.argsort(-feature_probabilities[1])[:50]#在出现概率从大到小的特征值中找到前50个特征值（输出为对应的数组下标）\n",
    "dv = model.named_steps['vectorize']\n",
    "for i , index_featrue in enumerate(top_feature):\n",
    "    print(i , dv.feature_names_[index_featrue] ,np.exp(feature_probabilities[1][index_featrue]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
